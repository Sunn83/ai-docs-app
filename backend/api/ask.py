from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import faiss, json, os, re
import numpy as np
from sentence_transformers import SentenceTransformer
from urllib.parse import quote
import requests

router = APIRouter()

INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"
PDF_BASE_URL = "http://144.91.115.48:8000/pdf"  # ÏƒÏ‰ÏƒÏ„ÏŒ path Î³Î¹Î± PDFs

LLAMA_URL = "http://llama:8080/completion"  # llama.cpp server

# ğŸ”¹ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÎºÎ±Î¹ index
model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
    raise RuntimeError("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ FAISS index Î® metadata.")

index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print("âœ… FAISS index ÎºÎ±Î¹ metadata Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÏƒÏ„Î· Î¼Î½Î®Î¼Î·.")

# -------------------- Memory Î³Î¹Î± follow-up --------------------
CHAT_HISTORY = []
MAX_HISTORY = 8

class Query(BaseModel):
    question: str


# -------------------- Utility: Clean text --------------------
def clean_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()


# -------------------- Build LLM prompt --------------------
def build_prompt(history, user_message, context_chunks):
    history_text = "".join(f"{role.upper()}: {content}\n" for role, content in history)
    context_text = "\n\n---\n\n".join(context_chunks)

    return f"""
Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î·Ï„Î¹ÎºÏŒ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ ÎµÎ¯ÏƒÎ±Î¹ Î½Î¿Î¼Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î¿Ï‚ ÏƒÎµ Î¦Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ® Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±, ÎšÎ¦Î”, ÎšÎ¦Î• ÎºÎ±Î¹ Î•Î›Î .

Î‘ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ ÏƒÏ…Î¶Î®Ï„Î·ÏƒÎ·Ï‚:
{history_text}

---

Î•ÏÏÏ„Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÏ„Î·:
USER: {user_message}

---

Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ (RAG):
{context_text}

ÎŸÎ´Î·Î³Î¯ÎµÏ‚:
- Î‘Î½ Î· ÎµÏÏÏ„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ follow-up, Î»Î¬Î²Îµ Ï…Ï€ÏŒÏˆÎ· Ï„Î¿ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ.
- Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ context, Ï€ÎµÏ‚ Â«Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Â».
- Î”ÏÏƒÎµ ÎºÎ±Î¸Î±ÏÎ®, Î´Î¿Î¼Î·Î¼Î­Î½Î· ÎºÎ±Î¹ Ï„ÎµÎºÎ¼Î·ÏÎ¹Ï‰Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.
"""


# -------------------- LLM call (local llama.cpp server) --------------------
def call_llm(prompt: str) -> str:
    payload = {
        "prompt": prompt,
        "n_predict": 512,
        "temperature": 0.2,
        "stop": ["USER:", "ASSISTANT:"]
    }

    try:
        r = requests.post(LLAMA_URL, json=payload, timeout=120)
        r.raise_for_status()
        data = r.json()
        return data.get("content", "").strip()
    except Exception as e:
        return f"âš  Î£Ï†Î¬Î»Î¼Î± Î±Ï€ÏŒ Ï„Î¿ LLM: {str(e)}"


# -------------------- API Endpoint --------------------
@router.post("/api/ask")
def ask(query: Query):
    try:
        question = query.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="Î†Î´ÎµÎ¹Î± ÎµÏÏÏ„Î·ÏƒÎ·.")

        # ğŸ”¹ Encode Query
        q_emb = model.encode([question], convert_to_numpy=True).astype("float32")
        faiss.normalize_L2(q_emb)

        # ğŸ”¹ FAISS Search
        k = 10
        D, I = index.search(q_emb, k)

        results = []
        for idx, score in zip(I[0], D[0]):
            if idx < len(metadata):
                md = metadata[idx]
                text = md.get("text", "").strip()
                if text:
                    results.append({
                        "idx": int(idx),
                        "score": float(score),
                        "filename": md.get("filename", "unknown.pdf"),
                        "page": md.get("page", 1),
                        "text": text
                    })

        if not results:
            return {"answers": [{"answer": "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.", "score": 0}]}

        # ğŸ”¹ ÎšÏÎ±Ï„Î¬ Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ 3 ÎºÎ±Î»ÏÏ„ÎµÏÎµÏ‚
        top_results = sorted(results, key=lambda x: x["score"], reverse=True)[:3]
        context_chunks = [r["text"] for r in top_results]

        # ğŸ”¹ Î¦Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ prompt
        prompt = build_prompt(CHAT_HISTORY, question, context_chunks)

        # ğŸ”¹ LLM Answer (ÎµÎ´Ï Ï€Î»Î­Î¿Î½ Î¼Î¹Î»Î¬Î¼Îµ Î¼Îµ Ï„Î¿ llama.cpp)
        response_text = call_llm(prompt)

        # ğŸ”¹ Memory Updated
        CHAT_HISTORY.append(("user", question))
        CHAT_HISTORY.append(("assistant", response_text))
        if len(CHAT_HISTORY) > MAX_HISTORY:
            CHAT_HISTORY[:] = CHAT_HISTORY[-MAX_HISTORY:]

        # ğŸ”¹ Î Î±ÎºÎµÏ„Î¬ÏÎ¹ÏƒÎ¼Î± Î±Ï€Î±Î½Ï„Î®ÏƒÎµÏ‰Î½ Î¼Îµ PDF links
        answers = []
        for r in top_results:
            answer_text = clean_text(r["text"])
            filename_pdf = re.sub(r"\.docx?$", ".pdf", r["filename"], flags=re.IGNORECASE)
            encoded_filename = quote(filename_pdf)
            pdf_url = f"{PDF_BASE_URL}/{encoded_filename}#page={r['page']}"

            formatted = (
                f"{answer_text}\n\n"
                f"ğŸ“„ Î Î·Î³Î®: [{r['filename']}]({pdf_url})\n"
                f"ğŸ“‘ Î£ÎµÎ»Î¯Î´Î±: {r['page']}"
            )
            answers.append({"answer": formatted, "score": r["score"]})

        return {
            "answers": answers,
            "query": question,
            "llm_answer": response_text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
