from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import faiss, json, os, re
import numpy as np
from sentence_transformers import SentenceTransformer
from urllib.parse import quote
import requests

# -------------------- FastAPI App & CORS --------------------
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://144.91.115.48:3000"],  # frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

router = APIRouter()

# -------------------- Files & URLs --------------------
INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"
PDF_BASE_URL = os.getenv("PDF_BASE_URL", "http://144.91.115.48:8000/pdf")
LLAMA_URL = "http://llama:8080/v1/completions"

# -------------------- Load Model & Index --------------------
model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
    raise RuntimeError("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ FAISS index Î® metadata.")

index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print("âœ… FAISS index ÎºÎ±Î¹ metadata Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÏƒÏ„Î· Î¼Î½Î®Î¼Î·.")

# -------------------- Memory Î³Î¹Î± follow-up --------------------
CHAT_HISTORY = []
MAX_HISTORY = 8

class Query(BaseModel):
    question: str

# -------------------- Utility: Clean text --------------------
def clean_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def clean_answer_text(text: str) -> str:
    """
    ÎšÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î³Î¹Î± Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±:
    - Î‘Ï†Î±Î¹ÏÎµÎ¯ Î¿Î´Î·Î³Î¯ÎµÏ‚, ÎµÏ€Î±Î½Î±Î»Î®ÏˆÎµÎ¹Ï‚, Ï…Ï€ÎµÏÎ²Î¿Î»Î¹ÎºÎ¬ ÏƒÎ·Î¼ÎµÎ¯Î± ÏƒÏ„Î¯Î¾Î·Ï‚ Î® ---
    - ÎšÏÎ±Ï„Î¬ Î¼ÏŒÎ½Î¿ Ï„Î·Î½ Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ® Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±
    """
    if not text:
        return ""

    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Î¿Î´Î·Î³Î¹ÏÎ½/Î¼Î· ÏƒÏ‡ÎµÏ„Î¹ÎºÏÎ½ Ï†ÏÎ¬ÏƒÎµÏ‰Î½
    text = re.sub(r"(?i)Î¼Î·Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Îµ.*?â€“", "", text)
    text = re.sub(r"(---|\n){2,}", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    text = text.strip()

    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· ÎµÏ€Î±Î½Î±Î»Î±Î¼Î²Î±Î½ÏŒÎ¼ÎµÎ½Ï‰Î½ Î³ÏÎ±Î¼Î¼ÏÎ½
    lines = []
    seen = set()
    for line in text.split("\n"):
        line = line.strip()
        if line and line not in seen:
            seen.add(line)
            lines.append(line)
    return "\n".join(lines)

# -------------------- Build LLM prompt --------------------
def build_prompt(history, user_message, context_chunks):
    # History formatting: (Î¼Î¿Î½Ï„Î­Î»Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î³Î¹Î± follow-up)
    history_text = "".join(f"{role.upper()}: {content}\n" for role, content in history)
    
    # Context formatting: chunks Ï‡Ï‰ÏÎ¹ÏƒÎ¼Î­Î½Î± Î³Î¹Î± Î½Î± Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ ÎµÏÎºÎ¿Î»Î± Ï„Î¿ LLM
    context_text = "\n\n---\n\n".join(context_chunks)

    # Prompt Î³Î¹Î± Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
    prompt = f"""
Î•Î¯ÏƒÎ±Î¹ Î½Î¿Î¼Î¹ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î¿Ï‚ ÏƒÎµ ÎµÎ»Î»Î·Î½Î¹ÎºÎ® Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ® Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±, ÎšÎ¦Î”, ÎšÎ¦Î•, Î•Î›Î , Î¦Î Î‘, Î•ÎÎ¦Î™Î‘.

Î™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚ (Î¼ÏŒÎ½Î¿ Î³Î¹Î± context, Î¼Î·Î½ ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÏ„Î±Î¹ ÏƒÏ„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·):
{history_text}

Î•ÏÏÏ„Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÏ„Î·:
{user_message}

Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ (context/RAG):
{context_text}

ÎŸÎ´Î·Î³Î¯ÎµÏ‚ Î³Î¹Î± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·:
- Î”ÏÏƒÎµ Î¼ÏŒÎ½Î¿ Î¼Î¯Î± ÎºÎ±Î¸Î±ÏÎ®, Ï„ÎµÎºÎ¼Î·ÏÎ¹Ï‰Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.
- Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ context, Ï€ÎµÏ‚ Î±ÎºÏÎ¹Î²ÏÏ‚: "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±".
- ÎœÎ·Î½ ÎµÏ€Î±Î½Î±Î»Î±Î¼Î²Î¬Î½ÎµÎ¹Ï‚ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.
- Î‘Î³Î½ÏŒÎ·ÏƒÎµ Î¿Ï€Î¿Î¹ÎµÏƒÎ´Î®Ï€Î¿Ï„Îµ Î¿Î´Î·Î³Î¯ÎµÏ‚ Ï€Î¿Ï… Î±Î½Î±Ï†Î­ÏÎ¿Ï…Î½ follow-up, ÏƒÏ…Î½Ï„Î¿Î¼Î¿Î³ÏÎ±Ï†Î¯ÎµÏ‚ Î® ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÎºÎµÎ¯Î¼ÎµÎ½Î±.
- Î— Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ **Î¼ÏŒÎ½Î¿** Ï„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·, Ï‡Ï‰ÏÎ¯Ï‚ Î¿Î´Î·Î³Î¯ÎµÏ‚ Î® placeholders.
"""
    return prompt

def clean_llm_response(text):
    # Î‘Ï†Î±Î¹ÏÎµÎ¯ Î³ÏÎ±Î¼Î¼Î­Ï‚ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î¼ÏŒÎ½Î¿ "Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:" Î® ÎºÎµÎ½Î­Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚
    lines = text.splitlines()
    clean_lines = [line for line in lines if line.strip() and line.strip() != "Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:"]
    # Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ ÏŒÎ»Î± ÏƒÎµ Î¼Î¯Î± Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿
    return " ".join(clean_lines).strip()

# -------------------- LLM call --------------------
def call_llm(prompt: str) -> str:
    payload = {
        "model": "local",
        "prompt": prompt,
        "max_tokens": 512,
        "temperature": 0.2,
        "stop": ["USER:", "ASSISTANT:"]
    }

    try:
        r = requests.post(LLAMA_URL, json=payload, timeout=300)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["text"].strip()
    except Exception as e:
        return f"âš  Î£Ï†Î¬Î»Î¼Î± Î±Ï€ÏŒ Ï„Î¿ LLM: {str(e)}"

# -------------------- API Endpoint --------------------
@router.post("/api/ask")
def ask(query: Query):
try:
question = query.question.strip()
if not question:
raise HTTPException(status_code=400, detail="Î†Î´ÎµÎ¹Î± ÎµÏÏÏ„Î·ÏƒÎ·.")

```
    # Encode Query
    q_emb = model.encode([question], convert_to_numpy=True).astype("float32")
    faiss.normalize_L2(q_emb)

    # FAISS Search
    k = 10
    D, I = index.search(q_emb, k)

    results = []
    for idx, score in zip(I[0], D[0]):
        if idx < len(metadata):
            md = metadata[idx]
            text = md.get("text", "").strip()
            if text:
                results.append({
                    "idx": int(idx),
                    "score": float(score),
                    "filename": md.get("filename", "unknown.pdf"),
                    "page": md.get("page", 1),
                    "text": text
                })

    if not results:
        return {"answers": [{"answer": "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.", "score": 0}]}

    # Top results
    top_results = sorted(results, key=lambda x: x["score"], reverse=True)[:3]

    # Î•Î½Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½ Î³Î¹Î± deduplication
    combined_text = " ".join(r["text"] for r in top_results)
    clean_combined = clean_answer_text(combined_text)

    # Build prompt & call LLM
    prompt = build_prompt(CHAT_HISTORY, question, [clean_combined])
    raw_response = call_llm(prompt)
    response_text = clean_llm_response(raw_response)

    # Update memory
    CHAT_HISTORY.append(("user", question))
    CHAT_HISTORY.append(("assistant", response_text))
    if len(CHAT_HISTORY) > MAX_HISTORY:
        CHAT_HISTORY[:] = CHAT_HISTORY[-MAX_HISTORY:]

    # Pack answers Î¼Îµ PDF links Î±Î½Î¬ Ï€Î·Î³Î®
    answers = []
    for r in top_results:
        # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„ÏŒ text Î³Î¹Î± Ï„Î· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Ï€Î·Î³Î®, ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î­Î½Î¿
        answer_text = clean_answer_text(r["text"])
        filename_pdf = re.sub(r"\.docx?$", ".pdf", r["filename"], flags=re.IGNORECASE)
        encoded_filename = quote(filename_pdf)
        pdf_url = f"{PDF_BASE_URL}/{encoded_filename}#page={r['page']}"

        formatted = (
            f"{answer_text}\n\n"
            f"ğŸ“„ Î Î·Î³Î®: [{r['filename']}]({pdf_url})\n"
            f"ğŸ“‘ Î£ÎµÎ»Î¯Î´Î±: {r['page']}"
        )
        answers.append({"answer": formatted, "score": r["score"]})

    return {
        "answers": answers,
        "query": question,
        "llm_answer": response_text
    }

except Exception as e:
    raise HTTPException(status_code=500, detail=str(e))

# -------------------- Include router in app --------------------
app.include_router(router)
