# backend/api/ask.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import faiss, json, os, re
import numpy as np
from sentence_transformers import SentenceTransformer

router = APIRouter()
TOP_K = 3  # Ï€ÏŒÏƒÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ
INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"

# ğŸ”¹ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÎºÎ±Î¹ index
model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
    raise RuntimeError("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ FAISS index Î® metadata.")

index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print("âœ… FAISS index ÎºÎ±Î¹ metadata Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÏƒÏ„Î· Î¼Î½Î®Î¼Î·.")

class Query(BaseModel):
    question: str

# âœ… ÎÎ­Î± clean_text Ï€Î¿Ï… Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î¹Ï‚ Î±Î»Î»Î±Î³Î­Ï‚ Î³ÏÎ±Î¼Î¼Î®Ï‚
def clean_text(t: str) -> str:
    if not t:
        return ""
    # ÎœÎ·Î½ Î±Ï†Î±Î¹ÏÎµÎ¯Ï‚ newlines, Î¼ÏŒÎ½Î¿ ÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎµ Ï„Î± Ï€ÎµÏÎ¹Ï„Ï„Î¬
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")
    t = re.sub(r"[ \t]+", " ", t)   # ÎšÎ±Î¸Î¬ÏÎ¹ÏƒÎµ Î´Î¹Ï€Î»Î¬ ÎºÎµÎ½Î¬
    t = re.sub(r"\n{3,}", "\n\n", t)  # ÎœÎ·Î½ Î±Ï†Î®Î½ÎµÎ¹Ï‚ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 2 ÏƒÏ…Î½ÎµÏ‡ÏŒÎ¼ÎµÎ½Î± newlines
    return t.strip()

@router.post("/api/ask")
def ask(query: Query):
    try:
        question = query.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="Î†Î´ÎµÎ¹Î± ÎµÏÏÏ„Î·ÏƒÎ·.")

        # Encode query
        q_emb = model.encode([f"query: {question}"], convert_to_numpy=True)
        q_emb = q_emb.astype('float32')
        faiss.normalize_L2(q_emb)

        # Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· FAISS
        k = 7
        D, I = index.search(q_emb, k)

        results = []
        for idx, score in zip(I[0], D[0]):
            if idx < len(metadata):
                md = metadata[idx]
                results.append({
                    "idx": int(idx),
                    "score": float(score),
                    "filename": md["filename"],
                    "section_title": md.get("section_title"),
                    "section_idx": md.get("section_idx"),
                    "chunk_id": md.get("chunk_id"),
                    "text": md.get("text")
                })

        if not results:
            return {"answer": "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.", "source": None, "query": question, "matches": []}

        # Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· chunks Î±Î½Î¬ ÎµÎ½ÏŒÏ„Î·Ï„Î±
        merged_by_section = {}
        for r in results:
            key = (r["filename"], r.get("section_idx"))
            merged_by_section.setdefault(key, {"chunks": [], "scores": []})
            merged_by_section[key]["chunks"].append((r["chunk_id"], r["text"]))
            merged_by_section[key]["scores"].append(r["score"])

        merged_list = sorted(merged_list, key=lambda x: x["score"], reverse=True)
        top_answers = merged_list[:TOP_K]

        answers_for_json = []
        for a in top_answers:
            text_with_source = f"{a['text']}\n\nğŸ“„ Î Î·Î³Î®: {a['filename']}\nğŸ“‘ Section: {a.get('section_idx')} | Chunk: {a.get('chunk_id')}"
            answers_for_json.append({
                "text": text_with_source,
                "source": a['filename'],
                "section": a.get('section_idx'),
                "chunk_id": a.get('chunk_id')
            })

return {
    "answer": answers_for_json[0]["text"],  # Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï‰Ï‚ main
    "query": question,
    "answers": answers_for_json
}

        # Join Ï€Î¯Î½Î±ÎºÎ± ÏŒÏ„Î±Î½ Ï€ÏÎ¿Î·Î³ÎµÎ¯Ï„Î±Î¹ Î±Î½Î±Ï†Î¿ÏÎ¬
        join_phrases = ["ÎºÎ¬Ï„Ï‰Î¸Î¹ Ï€Î¯Î½Î±ÎºÎ±", "Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿ Ï€Î¯Î½Î±ÎºÎ±", "Î²Î»Î­Ï€Îµ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î¯Î½Î±ÎºÎ±:"]
        for i, m in enumerate(merged_list[:-1]):
            text_lower = m["text"].lower()
            next_chunk = merged_list[i + 1]["text"]
            if any(p in text_lower for p in join_phrases) and "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:" in next_chunk:
                merged_list[i]["text"] = m["text"].rstrip() + "\n\n" + next_chunk.strip()

        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· top 5
        merged_list = sorted(merged_list, key=lambda x: x["score"], reverse=True)
        top_answers = merged_list[:5]

        # Î ÏÏÏ„Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Î¼Îµ Î­Î½Î´ÎµÎ¹Î¾Î· Ï€Î·Î³Î®Ï‚
        best = top_answers[0]
        answer_text = f"ğŸ“„ Î Î·Î³Î®: {best['filename']}\n\n{best['text']}"

        MAX_CHARS = 4000
        if len(answer_text) > MAX_CHARS:
            answer_text = answer_text[:MAX_CHARS].rsplit(' ', 1)[0] + " ..."

        return {
            "answer": answer_text,
            "source": best["filename"],
            "query": question,
            "matches": top_answers
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
