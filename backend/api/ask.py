# backend/api/ask.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import faiss, json, os, re
import numpy as np
from sentence_transformers import SentenceTransformer

router = APIRouter()

INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"

model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
    raise RuntimeError("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ FAISS index Î® metadata.")

index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print("âœ… FAISS index ÎºÎ±Î¹ metadata Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ ÏƒÏ„Î· Î¼Î½Î®Î¼Î·.")

class Query(BaseModel):
    question: str

# âœ… ÎšÏÎ¬Ï„Î± Markdown Ï€Î¯Î½Î±ÎºÎµÏ‚ ÏŒÏ€Ï‰Ï‚ ÎµÎ¯Î½Î±Î¹
def clean_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")
    # ÎœÎ—Î Ï€ÎµÎ¹ÏÎ¬Î¶ÎµÎ¹Ï‚ pipe Î® markdown formatting
    t = re.sub(r"[ \t]+", " ", t)   # ÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎµ Ï€ÎµÏÎ¹Ï„Ï„Î¬ ÎºÎµÎ½Î¬
    t = re.sub(r"\n{3,}", "\n\n", t)  # ÏŒÏ‡Î¹ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 2 newlines
    return t.strip()

@router.post("/api/ask")
def ask(query: Query):
    ...
    # ğŸ”¹ Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· FAISS
    k = 7
    D, I = index.search(q_emb, k)

    results = []
    for idx, score in zip(I[0], D[0]):
        if idx < len(metadata):
            md = metadata[idx]
            results.append({
                "idx": int(idx),
                "score": float(score),
                "filename": md["filename"],
                "section_title": md.get("section_title"),
                "section_idx": md.get("section_idx"),
                "chunk_id": md.get("chunk_id"),
                "text": md.get("text")
            })

    if not results:
        return {"answer": "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.", "source": None, "query": question}

    # ğŸ”¹ Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· chunks Î±Î½Î¬ ÎµÎ½ÏŒÏ„Î·Ï„Î±
    merged_by_section = {}
    for r in results:
        key = (r["filename"], r.get("section_idx"))
        merged_by_section.setdefault(key, {"chunks": [], "scores": []})
        merged_by_section[key]["chunks"].append((r["chunk_id"], r["text"]))
        merged_by_section[key]["scores"].append(r["score"])

    merged_list = []
    for (fname, sidx), val in merged_by_section.items():
        sorted_chunks = [t for _, t in sorted(val["chunks"], key=lambda x: x[0])]
        joined = "\n\n".join(sorted_chunks)
        avg_score = float(sum(val["scores"]) / len(val["scores"]))
        merged_list.append({
            "filename": fname,
            "section_idx": sidx,
            "text": joined,
            "score": avg_score
        })

    # Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Ï„Î¬ score
    merged_list = sorted(merged_list, key=lambda x: x["score"], reverse=True)

    # âœ¨ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î­Ï‰Ï‚ 3 ÏƒÏ‡ÎµÏ„Î¹ÎºÏÎ½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    top_matches = []
    for m in merged_list[:3]:
        m["text"] = clean_text(m["text"])
        top_matches.append({
            "filename": m["filename"],
            "score": round(m["score"], 4),
            "text": m["text"]
        })

    best = top_matches[0]
    answer_text = best["text"]

    MAX_CHARS = 4000
    if len(answer_text) > MAX_CHARS:
        answer_text = answer_text[:MAX_CHARS].rsplit(' ', 1)[0] + " ..."

    return {
        "answer": answer_text,
        "source": best["filename"],
        "query": question,
        "matches": top_matches
    }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
