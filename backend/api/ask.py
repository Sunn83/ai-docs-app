from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import faiss, json, os, re
import numpy as np
from sentence_transformers import SentenceTransformer
from urllib.parse import quote
import requests

# -------------------- FastAPI App & CORS --------------------
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://144.91.115.48:3000"],  # frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

router = APIRouter()

# -------------------- Files & URLs --------------------
INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"
PDF_BASE_URL = os.getenv("PDF_BASE_URL", "http://144.91.115.48:8000/pdf")
LLAMA_URL = "http://llama:8080/v1/completions"

# -------------------- Load Model & Index --------------------
model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
    raise RuntimeError("‚ùå ŒîŒµŒΩ Œ≤œÅŒ≠Œ∏Œ∑Œ∫Œµ FAISS index ŒÆ metadata.")

index = faiss.read_index(INDEX_FILE)
with open(META_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

print("‚úÖ FAISS index Œ∫Œ±Œπ metadata œÜŒøœÅœÑœéŒ∏Œ∑Œ∫Œ±ŒΩ œÉœÑŒ∑ ŒºŒΩŒÆŒºŒ∑.")

# -------------------- Memory Œ≥ŒπŒ± follow-up --------------------
CHAT_HISTORY = []
MAX_HISTORY = 8

class Query(BaseModel):
    question: str

# -------------------- Utility: Clean text --------------------
def clean_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def clean_answer_text(text: str) -> str:
    """
    ŒöŒ±Œ∏Œ±œÅŒØŒ∂ŒµŒπ œÑŒø Œ∫ŒµŒØŒºŒµŒΩŒø Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑œÇ Œ≥ŒπŒ± œÄŒøŒªŒªŒ±œÄŒªŒ¨ Œ±œÄŒøœÑŒµŒªŒ≠œÉŒºŒ±œÑŒ±:
    - ŒëœÜŒ±ŒπœÅŒµŒØ ŒøŒ¥Œ∑Œ≥ŒØŒµœÇ, ŒµœÄŒ±ŒΩŒ±ŒªŒÆœàŒµŒπœÇ, œÖœÄŒµœÅŒ≤ŒøŒªŒπŒ∫Œ¨ œÉŒ∑ŒºŒµŒØŒ± œÉœÑŒØŒæŒ∑œÇ ŒÆ ---
    - ŒöœÅŒ±œÑŒ¨ ŒºœåŒΩŒø œÑŒ∑ŒΩ ŒøœÖœÉŒπŒ±œÉœÑŒπŒ∫ŒÆ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒ±
    """
    if not text:
        return ""

    # ŒëœÜŒ±ŒØœÅŒµœÉŒ∑ ŒøŒ¥Œ∑Œ≥ŒπœéŒΩ/ŒºŒ∑ œÉœáŒµœÑŒπŒ∫œéŒΩ œÜœÅŒ¨œÉŒµœâŒΩ
    text = re.sub(r"(?i)ŒºŒ∑ŒΩ œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒµŒØœÑŒµ.*?‚Äì", "", text)
    text = re.sub(r"(---|\n){2,}", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    text = text.strip()

    # ŒëœÜŒ±ŒØœÅŒµœÉŒ∑ ŒµœÄŒ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ±ŒΩœåŒºŒµŒΩœâŒΩ Œ≥œÅŒ±ŒºŒºœéŒΩ
    lines = []
    seen = set()
    for line in text.split("\n"):
        line = line.strip()
        if line and line not in seen:
            seen.add(line)
            lines.append(line)
    return "\n".join(lines)

# -------------------- Build LLM prompt --------------------
def build_prompt(history, user_message, context_chunks):
    # History formatting: (ŒºŒøŒΩœÑŒ≠ŒªŒø œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒµŒØ Œ≥ŒπŒ± follow-up)
    history_text = "".join(f"{role.upper()}: {content}\n" for role, content in history)
    
    # Context formatting: chunks œáœâœÅŒπœÉŒºŒ≠ŒΩŒ± Œ≥ŒπŒ± ŒΩŒ± Œ¥ŒπŒ±Œ≤Œ¨Œ∂ŒµŒπ ŒµœçŒ∫ŒøŒªŒ± œÑŒø LLM
    context_text = "\n\n---\n\n".join(context_chunks)

    # Prompt Œ≥ŒπŒ± œÑŒø ŒºŒøŒΩœÑŒ≠ŒªŒø
    prompt = f"""
ŒïŒØœÉŒ±Œπ ŒΩŒøŒºŒπŒ∫œåœÇ Œ≤ŒøŒ∑Œ∏œåœÇ ŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒøœÇ œÉŒµ ŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ œÜŒøœÅŒøŒªŒøŒ≥ŒπŒ∫ŒÆ ŒΩŒøŒºŒøŒ∏ŒµœÉŒØŒ±, ŒöŒ¶Œî, ŒöŒ¶Œï, ŒïŒõŒ†, Œ¶Œ†Œë, ŒïŒùŒ¶ŒôŒë.

ŒôœÉœÑŒøœÅŒπŒ∫œå œÉœÖŒΩŒøŒºŒπŒªŒØŒ±œÇ (ŒºœåŒΩŒø Œ≥ŒπŒ± context, ŒºŒ∑ŒΩ ŒµŒºœÜŒ±ŒΩŒØŒ∂ŒµœÑŒ±Œπ œÉœÑŒ∑ŒΩ Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑):
{history_text}

ŒïœÅœéœÑŒ∑œÉŒ∑ œáœÅŒÆœÉœÑŒ∑:
{user_message}

ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒØŒ∑œÉŒµ ŒºœåŒΩŒø œÑŒπœÇ œÄŒ±œÅŒ±Œ∫Œ¨œÑœâ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒµœÇ (context/RAG):
{context_text}

ŒüŒ¥Œ∑Œ≥ŒØŒµœÇ Œ≥ŒπŒ± Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑:
- ŒîœéœÉŒµ ŒºœåŒΩŒø ŒºŒØŒ± Œ∫Œ±Œ∏Œ±œÅŒÆ, œÑŒµŒ∫ŒºŒ∑œÅŒπœâŒºŒ≠ŒΩŒ∑ Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑.
- ŒëŒΩ Œ¥ŒµŒΩ œÖœÄŒ¨œÅœáŒµŒπ Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑ œÉœÑŒø context, œÄŒµœÇ Œ±Œ∫œÅŒπŒ≤œéœÇ: "ŒîŒµŒΩ Œ≤œÅŒ≠Œ∏Œ∑Œ∫Œµ œÉœáŒµœÑŒπŒ∫ŒÆ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒ±".
- ŒúŒ∑ŒΩ ŒµœÄŒ±ŒΩŒ±ŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπœÇ œÑŒ∑ŒΩ Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑.
- ŒëŒ≥ŒΩœåŒ∑œÉŒµ ŒøœÄŒøŒπŒµœÉŒ¥ŒÆœÄŒøœÑŒµ ŒøŒ¥Œ∑Œ≥ŒØŒµœÇ œÄŒøœÖ Œ±ŒΩŒ±œÜŒ≠œÅŒøœÖŒΩ follow-up, œÉœÖŒΩœÑŒøŒºŒøŒ≥œÅŒ±œÜŒØŒµœÇ ŒÆ ŒµœÄŒπœÄŒªŒ≠ŒøŒΩ Œ∫ŒµŒØŒºŒµŒΩŒ±.
- Œó Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑ œÄœÅŒ≠œÄŒµŒπ ŒΩŒ± ŒµŒØŒΩŒ±Œπ **ŒºœåŒΩŒø** œÑŒø œÑŒµŒªŒπŒ∫œå œÄŒµœÅŒπŒµœáœåŒºŒµŒΩŒø œÄœÅŒøœÇ œÑŒøŒΩ œáœÅŒÆœÉœÑŒ∑, œáœâœÅŒØœÇ ŒøŒ¥Œ∑Œ≥ŒØŒµœÇ ŒÆ placeholders.
"""
    return prompt

def clean_llm_response(text):
    # ŒëœÜŒ±ŒπœÅŒµŒØ Œ≥œÅŒ±ŒºŒºŒ≠œÇ œÄŒøœÖ œÄŒµœÅŒπŒ≠œáŒøœÖŒΩ ŒºœåŒΩŒø "ŒëœÄŒ¨ŒΩœÑŒ∑œÉŒ∑:" ŒÆ Œ∫ŒµŒΩŒ≠œÇ Œ≥œÅŒ±ŒºŒºŒ≠œÇ
    lines = text.splitlines()
    clean_lines = [line for line in lines if line.strip() and line.strip() != "ŒëœÄŒ¨ŒΩœÑŒ∑œÉŒ∑:"]
    # ŒïœÄŒπœÉœÑœÅŒ≠œÜŒµŒπ œåŒªŒ± œÉŒµ ŒºŒØŒ± œÄŒ±œÅŒ¨Œ≥œÅŒ±œÜŒø
    return " ".join(clean_lines).strip()

# -------------------- LLM call --------------------
def call_llm(prompt: str) -> str:
    payload = {
        "model": "local",
        "prompt": prompt,
        "max_tokens": 512,
        "temperature": 0.2,
        "stop": ["USER:", "ASSISTANT:"]
    }

    try:
        r = requests.post(LLAMA_URL, json=payload, timeout=300)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["text"].strip()
    except Exception as e:
        return f"‚ö† Œ£œÜŒ¨ŒªŒºŒ± Œ±œÄœå œÑŒø LLM: {str(e)}"

# -------------------- API Endpoint --------------------
@router.post("/api/ask")
def ask(query: Query):
    try:
        question = query.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="ŒÜŒ¥ŒµŒπŒ± ŒµœÅœéœÑŒ∑œÉŒ∑.")

        # Encode Query
        q_emb = model.encode([question], convert_to_numpy=True).astype("float32")
        faiss.normalize_L2(q_emb)

        # FAISS Search
        k = 10
        D, I = index.search(q_emb, k)

        results = []
        for idx, score in zip(I[0], D[0]):
            if idx < len(metadata):
                md = metadata[idx]
                text = md.get("text", "").strip()
                if text:
                    results.append({
                        "idx": int(idx),
                        "score": float(score),
                        "filename": md.get("filename", "unknown.pdf"),
                        "page": md.get("page", 1),
                        "text": text
                    })

        if not results:
            return {"answers": [{"answer": "ŒîŒµŒΩ Œ≤œÅŒ≠Œ∏Œ∑Œ∫Œµ œÉœáŒµœÑŒπŒ∫ŒÆ Œ±œÄŒ¨ŒΩœÑŒ∑œÉŒ∑.", "score": 0}]}

        top_results = sorted(results, key=lambda x: x["score"], reverse=True)[:3]
        context_chunks = [r["text"] for r in top_results]

        # Build prompt & call LLM
        prompt = build_prompt(CHAT_HISTORY, question, context_chunks)
        raw_response = call_llm(prompt)
        response_text = clean_llm_response(raw_response)

        # Update memory
        CHAT_HISTORY.append(("user", question))
        CHAT_HISTORY.append(("assistant", response_text))
        if len(CHAT_HISTORY) > MAX_HISTORY:
            CHAT_HISTORY[:] = CHAT_HISTORY[-MAX_HISTORY:]

        # Pack answers with PDF links
        answers = []
        for r in top_results:
            answer_text = clean_answer_text(r["text"])
            filename_pdf = re.sub(r"\.docx?$", ".pdf", r["filename"], flags=re.IGNORECASE)
            encoded_filename = quote(filename_pdf)
            pdf_url = f"{PDF_BASE_URL}/{encoded_filename}#page={r['page']}"

            formatted = (
                f"{answer_text}\n\n"
                f"üìÑ Œ†Œ∑Œ≥ŒÆ: [{r['filename']}]({pdf_url})\n"
                f"üìë Œ£ŒµŒªŒØŒ¥Œ±: {r['page']}"
            )
            answers.append({"answer": formatted, "score": r["score"]})

        return {
            "answers": answers,
            "query": question,
            "llm_answer": response_text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# -------------------- Include router in app --------------------
app.include_router(router)
