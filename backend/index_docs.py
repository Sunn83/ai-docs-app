# backend/index_docs.py
import os
import json
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import faiss
import re

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ chunking
CHUNK_SIZE = 350  # Î»Î­Î¾ÎµÎ¹Ï‚ Î±Î½Î¬ chunk
CHUNK_OVERLAP = 50  # ÎµÏ€Î¹ÎºÎ¬Î»Ï…ÏˆÎ·

def read_docx(file_path):
    doc = Document(file_path)
    parts = []
    current_heading = None

    for p in doc.paragraphs:
        text = p.text.strip()
        if not text:
            continue

        style_name = ""
        try:
            style_name = getattr(p.style, "name", "") or ""
        except Exception:
            pass

        # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ ÎµÎ¯Î½Î±Î¹ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î± (Î±Î³Î³Î»Î¹ÎºÎ¬ Î® ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬)
        if "heading" in style_name.lower() or "ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î±" in style_name.lower():
            current_heading = text
            continue

        # Î‘Î½ Î­Ï‡Î¿Ï…Î¼Îµ heading, Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î¼Îµ prefix
        if current_heading:
            parts.append(f"{current_heading}: {text}")
        else:
            parts.append(text)

    return "\n".join(parts)

import re

def chunk_text(text, chunk_size=350, overlap=50):
    """
    Î£Ï€Î¬ÎµÎ¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÎµ chunks ~350 Î»Î­Î¾ÎµÏ‰Î½ Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± ÎºÏŒÎ²ÎµÎ¹ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.
    Î”Î¹Î±Ï„Î·ÏÎµÎ¯ overlap (ÎµÏ€Î¹ÎºÎ¬Î»Ï…ÏˆÎ·) Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ chunks Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ± embeddings.
    """
    # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï€ÎµÏÎ¹Ï„Ï„ÏÎ½ newlines / Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ ÎºÎµÎ½ÏÎ½
    text = re.sub(r'\s+', ' ', text.strip())

    # Î£Ï€Î¬ÏƒÎ¹Î¼Î¿ ÏƒÎµ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ (Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï…Ï€ÏŒÏˆÎ· ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ ÏƒÎ·Î¼ÎµÎ¯Î± ÏƒÏ„Î¯Î¾Î·Ï‚)
    sentences = re.split(r'(?<=[.!;?])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    chunks = []
    current_chunk = []
    current_len = 0

    for sent in sentences:
        words = sent.split()
        sent_len = len(words)

        if current_len + sent_len > chunk_size:
            # Î”Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ Î½Î­Î¿ chunk
            chunks.append(" ".join(current_chunk))

            # ÎšÏÎ¬Ï„Î± overlap (Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚ Î»Î­Î¾ÎµÎ¹Ï‚ Î±Ï€ÏŒ Ï„Î¿ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿)
            overlap_text = " ".join(" ".join(current_chunk).split()[-overlap:])
            current_chunk = [overlap_text, sent]
            current_len = len(overlap_text.split()) + sent_len
        else:
            current_chunk.append(sent)
            current_len += sent_len

    # Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ chunk
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

def load_docs():
    metadata = []
    all_chunks = []
    for fname in os.listdir(DOCS_PATH):
        if not fname.lower().endswith(".docx"):
            continue
        path = os.path.join(DOCS_PATH, fname)
        text = read_docx(path)
        chunks = chunk_text(text)
        for idx, chunk in enumerate(chunks):
            metadata.append({
                "filename": fname,
                "chunk_id": idx,
                "text": chunk
            })
            all_chunks.append(chunk)
    return all_chunks, metadata

def create_faiss_index(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

def main():
    print("ğŸ“„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")
    chunks, metadata = load_docs()
    print(f"â¡ï¸  Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(chunks)} chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")

    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

    print("ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...")
    embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)

    print("ğŸ’¾ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)

    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")

if __name__ == "__main__":
    main()
