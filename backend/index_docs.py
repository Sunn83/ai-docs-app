# backend/index_docs.py
import os
import json
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

# ğŸ“ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ chunking (Ï€Î¹Î¿ Î¼ÎµÎ³Î¬Î»Î± chunks Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ± context)
CHUNK_SIZE = 500
CHUNK_OVERLAP = 150

# âœ… ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ Markdown
def table_to_markdown(table, wrap_length=90):
    def wrap_text(text, max_length=wrap_length):
        words = text.split()
        lines, current = [], ""
        for word in words:
            if len(current) + len(word) + 1 > max_length:
                lines.append(current)
                current = word
            else:
                current += (" " if current else "") + word
        if current:
            lines.append(current)
        # âš™ï¸ Î±Î½Ï„Î¯ Î³Î¹Î± <br> Î²Î¬Î»Îµ Î±Ï€Î»ÏŒ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î± (Î³Î¹Î± Î½Î± Î¼Î·Î½ ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ frontend)
        return " ".join(lines)

    rows_text = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            text = cell.text.strip()
            text = text.replace("\u00A0", " ").replace("\r", "").replace("\n", " ")
            text = wrap_text(text)
            cells.append(text)
        rows_text.append(" | ".join(cells))

    if not rows_text:
        return ""

    num_cols = rows_text[0].count("|") + 1
    separator = " | ".join(["---"] * num_cols)

    markdown_table = "\n".join([
        "",
        "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:",
        rows_text[0],
        separator,
        *rows_text[1:],
        ""
    ])
    return markdown_table

# âœ… Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· docx Î¼Îµ Î´Î¿Î¼Î® ÎºÎ±Î¹ Ï€Î¯Î½Î±ÎºÎµÏ‚
def read_docx_sections(filepath):
    doc = Document(filepath)
    sections = []
    current_title = None
    current_body = []

    def flush_section():
        nonlocal current_title, current_body
        if not current_title and not current_body:
            return
        text = "\n".join([t.strip() for t in current_body if t.strip()])
        if text.strip():
            sections.append({
                "title": current_title.strip() if current_title else None,
                "text": text.strip()
            })
        current_title = None
        current_body = []

    for block in doc.element.body:
        if block.tag.endswith("p"):
            txt = block.text.strip() if hasattr(block, "text") else ""
            if not txt:
                continue
            style = ""
            try:
                style = block.style.name.lower()
            except Exception:
                pass
            if style.startswith("heading") or "ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î±" in style or re.match(r"^\s*(Î¬ÏÎ¸ÏÎ¿|ÎµÎ½ÏŒÏ„Î·Ï„Î±|Î¸Î­Î¼Î±|\d+(\.\d+)+)", txt.lower()):
                flush_section()
                current_title = txt
                continue
            current_body.append(txt)

        elif block.tag.endswith("tbl"):
            try:
                table = next(t for t in doc.tables if t._element == block)
            except StopIteration:
                continue
            table_md = table_to_markdown(table)
            if table_md.strip():
                current_body.append(table_md)

    flush_section()
    if not sections:
        all_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        sections = [{"title": None, "text": all_text}]
    return sections

def chunk_section_text(section_text, max_words=500, overlap_words=100):
    """
    Î£Ï€Î¬ÎµÎ¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÎµ chunks ÎœÎŸÎÎŸ ÎµÎºÏ„ÏŒÏ‚ markdown Ï€Î¹Î½Î¬ÎºÏ‰Î½.
    Î‘Î½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÎµÎ¹ Ï†ÏÎ¬ÏƒÎ· Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎ± ("ÎºÎ¬Ï„Ï‰Î¸Î¹ Ï€Î¯Î½Î±ÎºÎ±", "Î²Î»Î­Ï€Îµ Ï€Î¯Î½Î±ÎºÎ±" Îº.Î»Ï€.)
    Ï€ÏÎ¹Î½ Î±Ï€ÏŒ Ï„Î¿Î½ Ï€Î¯Î½Î±ÎºÎ±, Ï„Î¿Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÎ½ÎµÎ¹ ÏƒÏ„Î¿ Î¯Î´Î¹Î¿ chunk.
    """
    if not section_text:
        return []

    # Î”Î¹Î¬ÏƒÏ€Î±ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· Ï€Î¯Î½Î±ÎºÎµÏ‚
    parts = re.split(r'(?=ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:)', section_text)
    chunks = []
    prev_part = ""

    join_triggers = ["Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î¯Î½Î±ÎºÎ±Ï‚", "ÎºÎ¬Ï„Ï‰Î¸Î¹ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€Î¯Î½Î±ÎºÎ±", "Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿ Ï€Î¯Î½Î±ÎºÎ±", "Î²Î»Î­Ï€Îµ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î¯Î½Î±ÎºÎ±:"]

    for part in parts:
        part = part.strip()
        if not part:
            continue

        # Î‘Î½ Ï„Î¿ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹ ÎµÎ¯Î½Î±Î¹ Ï€Î¯Î½Î±ÎºÎ±Ï‚
        if part.startswith("ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:"):
            # â• Î‘Î½ Ï„Î¿ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿ Î±Î½Î±Ï†Î­ÏÎµÎ¹ Ï€Î¯Î½Î±ÎºÎ±, ÏƒÏ…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ­ Ï„Î±
            if prev_part and any(trig in prev_part.lower() for trig in join_triggers):
                prev_part = prev_part.rstrip() + "\n\n" + part.strip()
                chunks[-1] = prev_part
                prev_part = ""
            else:
                chunks.append(part)
            continue

        # ÎšÎ±Î½Î¿Î½Î¹ÎºÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ â€” split ÏƒÎµ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚
        sentences = re.split(r'(?<=[\.\!\?])\s+', part)
        cur, cur_count = [], 0

        for s in sentences:
            wcount = len(s.split())
            if cur_count + wcount > max_words and cur:
                joined = " ".join(cur).strip()
                chunks.append(joined)
                tail = " ".join(" ".join(cur).split()[-overlap_words:])
                cur = [tail, s]
                cur_count = len(tail.split()) + wcount
            else:
                cur.append(s)
                cur_count += wcount

        if cur:
            joined = " ".join(cur).strip()
            chunks.append(joined)
            prev_part = joined  # Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎµ Î³Î¹Î± Ï€Î¹Î¸Î±Î½ÏŒ Ï€Î¯Î½Î±ÎºÎ± Î¼ÎµÏ„Î¬

    # ÎšÎ±Î¸Î¬ÏÎ¹ÏƒÎµ Î¼Î¹ÎºÏÎ¬/ÎºÎµÎ½Î¬ chunks
    chunks = [c for c in chunks if len(c.split()) > 5]
    return chunks

def load_docs():
    metadata, all_chunks = [], []
    for fname in os.listdir(DOCS_PATH):
        if not fname.lower().endswith(".docx"):
            continue
        path = os.path.join(DOCS_PATH, fname)
        sections = read_docx_sections(path)
        for si, sec in enumerate(sections):
            sec_title = sec.get("title")
            sec_text = sec.get("text") or ""
            chunks = chunk_section_text(sec_text, max_words=CHUNK_SIZE, overlap_words=CHUNK_OVERLAP)
            if not chunks:
                if sec_text.strip():
                    chunks = [sec_text.strip()]
            for cj, chunk in enumerate(chunks):
                metadata.append({
                    "filename": fname,
                    "section_title": sec_title,
                    "section_idx": si,
                    "chunk_id": cj,
                    "text": chunk
                })
                all_chunks.append(chunk)
    return all_chunks, metadata

def create_faiss_index(embeddings):
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index

def main():
    print("ğŸ“„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")
    chunks, metadata = load_docs()
    print(f"â¡ï¸  Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(chunks)} chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")
    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")
    print("ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...")
    embeddings = model.encode([f"passage: {c}" for c in chunks], convert_to_numpy=True, show_progress_bar=True)
    embeddings = embeddings.astype('float32')
    print("ğŸ”§ ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· embeddings (L2) + Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")

if __name__ == "__main__":
    main()
