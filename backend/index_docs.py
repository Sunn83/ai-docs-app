# backend/index_docs.py
import os
import json
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ chunking
CHUNK_SIZE = 350  # Î»Î­Î¾ÎµÎ¹Ï‚ Î±Î½Î¬ chunk
CHUNK_OVERLAP = 50  # ÎµÏ€Î¹ÎºÎ¬Î»Ï…ÏˆÎ·

# --- section-aware reading & chunking (Î²Î¬Î»Îµ ÏƒÏ„Î¿ backend/index_docs.py) ---
from docx import Document
import re

def read_docx_sections(file_path):
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î»Î¯ÏƒÏ„Î± sections: ÎºÎ¬Î¸Îµ section = {"title": title_or_none, "text": full_text}
    Î ÏÎ¿ÏƒÏ€Î±Î¸ÎµÎ¯ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹ paragraph styles (Heading...), Î±Î»Î»Î¹ÏÏ‚ Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½,
    Î±Î½Î¹Ï‡Î½ÎµÏÎµÎ¹ Î³ÏÎ±Î¼Î¼Î­Ï‚ Ï€Î¿Ï… Ï„ÎµÏÎ¼Î±Ï„Î¯Î¶Î¿Ï…Î½ ÏƒÎµ ':' Î® ÎµÎ¯Î½Î±Î¹ ÏƒÏÎ½Ï„Î¿Î¼ÎµÏ‚ Ï‰Ï‚ Ï„Î¯Ï„Î»Î¿Ï…Ï‚.
    """
    doc = Document(file_path)
    sections = []
    current_title = None
    current_body = []

    def flush_section():
        if current_title is None and not current_body:
            return
        text = "\n".join([t for t in current_body if t.strip()])
        sections.append({
            "title": current_title.strip() if current_title else None,
            "text": text.strip()
        })

    for p in doc.paragraphs:
        txt = p.text.strip()
        if not txt:
            # blank line -> skip but keep in body
            continue

        # case 1: style indicates heading (Heading 1/2/3) â€” ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ "Heading 1" ÎºÎ»Ï€
        style_name = getattr(p.style, "name", "") or ""
        if style_name.lower().startswith("heading") or style_name.lower().startswith("ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î±"):
            # new section
            if current_title is not None or current_body:
                flush_section()
            current_title = txt
            current_body = []
            continue

        # case 2: fallback heading detection (short line or endswith ':')
        if (len(txt.split()) <= 8 and txt.endswith(":")) or (len(txt) <= 60 and len(txt.split()) <= 5 and txt.endswith(":")):
            # treat as title
            if current_title is not None or current_body:
                flush_section()
            current_title = txt
            current_body = []
            continue

        # else: normal paragraph -> append to body
        current_body.append(txt)

    # flush last
    if current_title is not None or current_body:
        flush_section()

    # If file has NO headings at all, create 1 section with whole text
    if not sections:
        # fallback: join paragraphs into single section
        doc_text = "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
        sections = [{"title": None, "text": doc_text}]

    return sections

def chunk_section_text(section_text, max_words=400, overlap_words=60):
    """
    ÎœÎµ Î²Î¬ÏƒÎ· Î»Î­Î¾ÎµÎ¹Ï‚ - ÏƒÏ€Î¬ÎµÎ¹ Ï„Î· section ÏƒÎµ chunks, ÎºÏÎ±Ï„ÏÎ½Ï„Î±Ï‚ sentences Î±ÎºÎ­ÏÎ±Î¹ÎµÏ‚.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î»Î¯ÏƒÏ„Î± chunk strings.
    """
    if not section_text:
        return []

    # split ÏƒÎµ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ (Î²Î±ÏƒÎ¹ÎºÎ¬ Î¼Îµ ., ?, ! Î±Î»Î»Î¬ Î´Î¹Î±Ï„Î·ÏÎ¿ÏÎ¼Îµ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬)
    sentences = re.split(r'(?<=[\.\!\?])\s+', section_text.strip())
    chunks = []
    cur = []
    cur_count = 0

    for s in sentences:
        words = s.split()
        wcount = len(words)
        if cur_count + wcount > max_words and cur:
            chunks.append(" ".join(cur).strip())
            # overlap: keep last overlap_words words from cur
            tail = " ".join(" ".join(cur).split()[-overlap_words:])
            cur = [tail, s]
            cur_count = len(tail.split()) + wcount
        else:
            cur.append(s)
            cur_count += wcount

    if cur:
        chunks.append(" ".join(cur).strip())

    # dedupe empty and very short
    chunks = [c for c in chunks if len(c.split()) > 5]
    return chunks

def load_docs():
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: chunks_list, metadata_list (ordered lists)
    metadata entries: {"filename": fname, "section_title": title, "section_idx": i_section, "chunk_id": j_chunk}
    """
    metadata = []
    all_chunks = []
    for fname in os.listdir(DOCS_PATH):
        if not fname.lower().endswith(".docx"):
            continue
        path = os.path.join(DOCS_PATH, fname)
        sections = read_docx_sections(path)
        for si, sec in enumerate(sections):
            sec_title = sec.get("title")
            sec_text = sec.get("text") or ""
            # split section to chunks
            chunks = chunk_section_text(sec_text, max_words=CHUNK_SIZE, overlap_words=CHUNK_OVERLAP)
            if not chunks:
                # if section was too small, keep whole section text
                if sec_text.strip():
                    chunks = [sec_text.strip()]
            for cj, chunk in enumerate(chunks):
                metadata.append({
                    "filename": fname,
                    "section_title": sec_title,
                    "section_idx": si,
                    "chunk_id": cj,
                    "text": chunk
                })
                all_chunks.append(chunk)
    return all_chunks, metadata


def split_by_headings(text):
    """
    Î£Ï€Î¬ÎµÎ¹ Ï„Î¿ docx ÏƒÎµ ÎµÎ½ÏŒÏ„Î·Ï„ÎµÏ‚ Î¼Îµ Î²Î¬ÏƒÎ· ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚ Ï„ÏÏ€Î¿Ï… '2.4 ...' Î® 'Î†ÏÎ¸ÏÎ¿ ...'
    """
    # ÎšÎ±Î½Î¿Î½Î¹ÎºÎ® Î­ÎºÏ†ÏÎ±ÏƒÎ· Ï€Î¿Ï… ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚ (Ï€.Ï‡. 2.4, 3.1, Î†ÏÎ¸ÏÎ¿ 5, Î˜Î­Î¼Î±)
    pattern = re.compile(r'(?=\n?\s*(?:\d+\.\d+|Î†ÏÎ¸ÏÎ¿\s+\d+|Î˜Î­Î¼Î±|Î•Î½ÏŒÏ„Î·Ï„Î±)\b)', re.IGNORECASE)
    parts = pattern.split(text)
    return [p.strip() for p in parts if len(p.strip()) > 50]  # Î±Î³Î½ÏŒÎ·ÏƒÎµ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÎ¬

def create_faiss_index(embeddings):
    # normalize Î³Î¹Î± cosine similarity
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)   # inner product (cosine if normalized)
    index.add(embeddings)
    return index

def main():
    print("ğŸ“„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")
    chunks, metadata = load_docs()
    print(f"â¡ï¸  Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(chunks)} chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")

    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

    print("ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...")
    embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)

    # convert to float32 if ÏŒÏ‡Î¹ Î®Î´Î·
    embeddings = embeddings.astype('float32')

    print("ğŸ”§ ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· embeddings (L2) + Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)

    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")


if __name__ == "__main__":
    main()
