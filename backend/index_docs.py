# backend/index_docs.py
import os
import json
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ chunking
CHUNK_SIZE = 350  # Î»Î­Î¾ÎµÎ¹Ï‚ Î±Î½Î¬ chunk
CHUNK_OVERLAP = 50  # ÎµÏ€Î¹ÎºÎ¬Î»Ï…ÏˆÎ·

# --- section-aware reading & chunking (Î²Î¬Î»Îµ ÏƒÏ„Î¿ backend/index_docs.py) ---

from docx import Document
import re

# âœ… ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ markdown Î¼Îµ wrap Î¼ÎµÎ³Î¬Î»Ï‰Î½ ÎºÎµÎ»Î¹ÏÎ½ (Ï‡Ï‰ÏÎ¯Ï‚ <br>)
def table_to_markdown(table, wrap_length=80):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î­Î½Î±Î½ DOCX Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ Markdown.
    Î“Î¹Î± Î½Î± ÏƒÏ€Î¬ÏƒÎ¿Ï…Î¼Îµ Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Î³ÏÎ±Î¼Î¼Î­Ï‚, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ hard line breaks: Î´ÏÎ¿ ÎºÎµÎ½Î¬ + \n
    (ReactMarkdown + remark-gfm Î¸Î± Ï„Î± ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÎ¹ ÏƒÏ‰ÏƒÏ„Î¬).
    """
    def wrap_text(text, max_length=wrap_length):
        words = text.split()
        lines = []
        current = ""
        for word in words:
            if len(current) + len(word) + (1 if current else 0) > max_length:
                lines.append(current)
                current = word
            else:
                current += (" " if current else "") + word
        if current:
            lines.append(current)
        # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Î´ÏÎ¿ ÎºÎµÎ½Î¬ Ï€ÏÎ¹Î½ Ï„Î¿ newline Î³Î¹Î± hard break ÏƒÎµ Markdown
        return ("  \n").join([ln.strip() for ln in lines if ln.strip()])

    rows_text = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            text = cell.text.strip()
            # ÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ¼Î± Î±ÎºÎ±Ï„Î¬Î»Î»Î·Î»Ï‰Î½ whitespace
            text = text.replace("\u00A0", " ").replace("\r", " ").replace("\n", " ")
            text = re.sub(r"\s+", " ", text).strip()
            text = wrap_text(text)
            cells.append(text)
        rows_text.append(" | ".join(cells))

    if not rows_text:
        return ""

    num_cols = rows_text[0].count("|") + 1
    separator = " | ".join(["---"] * num_cols)

    markdown_table = "\n".join([
        "",
        "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:",
        rows_text[0],
        separator,
        *rows_text[1:],
        ""
    ])

    return markdown_table


# âœ… ÎÎ­Î± Î±ÏƒÏ†Î±Î»Î®Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚
def read_docx_sections(filepath):
    """
    Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î¿ DOCX Î¼Îµ Ï€Î»Î®ÏÎ· ÏƒÎµÎ¹ÏÎ¬ (Ï€Î±ÏÎ¬Î³ÏÎ±Ï†Î¿Î¹ + Ï€Î¯Î½Î±ÎºÎµÏ‚)
    ÎºÎ±Î¹ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ ÎºÎ±Î¸Î±ÏÎ­Ï‚ ÎµÎ½ÏŒÏ„Î·Ï„ÎµÏ‚ Ï‡Ï‰ÏÎ¯Ï‚ ÎµÏ€Î±Î½Î±Î»Î®ÏˆÎµÎ¹Ï‚.
    """
    doc = Document(filepath)
    sections = []
    current_title = None
    current_body = []

    def flush_section():
        nonlocal current_title, current_body
        if not current_title and not current_body:
            return
        text = "\n".join([t.strip() for t in current_body if t.strip()])
        if text.strip():
            sections.append({
                "title": current_title.strip() if current_title else None,
                "text": text.strip()
            })
        current_title = None
        current_body = []

    # ğŸ”¹ Î”Î¹Î¬Î²Î±ÏƒÎµ Ï„Î· Î´Î¿Î¼Î® Ï„Î¿Ï… docx Î¼Îµ Ï„Î· ÏƒÏ‰ÏƒÏ„Î® ÏƒÎµÎ¹ÏÎ¬
    for block in doc.element.body:
        if block.tag.endswith("p"):
            paragraph = block
            txt = paragraph.text.strip() if hasattr(paragraph, "text") else ""
            if not txt:
                continue

            # Î‘Î½ Î­Ï‡ÎµÎ¹Ï‚ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î± (Ï€.Ï‡. "Î†ÏÎ¸ÏÎ¿", "Î˜Î­Î¼Î±", "Î•Î½ÏŒÏ„Î·Ï„Î±")
            style = ""
            try:
                style = paragraph.style.name.lower()
            except Exception:
                pass

            if style.startswith("heading") or "ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î±" in style:
                flush_section()
                current_title = txt
                continue

            if re.match(r"^\s*(Î¬ÏÎ¸ÏÎ¿|ÎµÎ½ÏŒÏ„Î·Ï„Î±|Î¸Î­Î¼Î±|\d+(\.\d+)+)", txt.lower()):
                flush_section()
                current_title = txt
                continue

            current_body.append(txt)

        elif block.tag.endswith("tbl"):
            try:
                table = next(t for t in doc.tables if t._element == block)
            except StopIteration:
                continue
            table_md = table_to_markdown(table)
            if table_md.strip():
                current_body.append(table_md)

    flush_section()

    if not sections:
        all_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        sections = [{"title": None, "text": all_text}]

    return sections

def chunk_section_text(section_text, max_words=400, overlap_words=60):
    """
    Î£Ï€Î¬ÎµÎ¹ section_text ÏƒÎµ chunks, Î±Î»Î»Î¬ **Î´ÎµÎ½ ÎºÏŒÎ²ÎµÎ¹** Î¼Î­ÏƒÎ± ÏƒÎµ markdown Ï€Î¯Î½Î±ÎºÎµÏ‚.
    Î•Î¾Î¬Î³ÎµÎ¹ Ï€ÏÏÏ„Î± ÎºÎ¬Î¸Îµ 'ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:' block Ï‰Ï‚ Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„ÏŒ chunk.
    Î¤Î¿ Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÏ€Î¬ÎµÎ¹ ÏƒÎµ chunks Î¼Îµ Î²Î¬ÏƒÎ· Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.
    """
    if not section_text:
        return []

    chunks = []
    # pattern Ï€Î¿Ï… Î²ÏÎ¯ÏƒÎºÎµÎ¹ ÎºÎ¬Î¸Îµ Ï€Î¯Î½Î±ÎºÎ± Ï€Î¿Ï… Î¾ÎµÎºÎ¹Î½Î¬ Î¼Îµ "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:" Î­Ï‰Ï‚ Ï€ÏÎ¹Î½ Ï„Î¿Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ Î® EOF
    table_pattern = re.compile(r'ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:\n.*?(?=(?:\nğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:)|\Z)', re.S)

    cursor = 0
    for m in table_pattern.finditer(section_text):
        start, end = m.span()
        # ÎºÎ¿Î¼Î¼Î¬Ï„Î¹ Ï€ÏÎ¹Î½ Ï„Î¿Î½ Ï€Î¯Î½Î±ÎºÎ± -> Ï„Î¿ ÏƒÏ€Î¬Î¼Îµ
        pre = section_text[cursor:start].strip()
        if pre:
            # split ÏƒÎµ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ chunk
            sentences = re.split(r'(?<=[\.\!\?])\s+', pre)
            cur, cur_count = [], 0
            for s in sentences:
                wcount = len(s.split())
                if cur_count + wcount > max_words and cur:
                    chunks.append(" ".join(cur).strip())
                    tail = " ".join(" ".join(cur).split()[-overlap_words:])
                    cur = [tail, s]
                    cur_count = len(tail.split()) + wcount
                else:
                    cur.append(s)
                    cur_count += wcount
            if cur:
                chunks.append(" ".join(cur).strip())

        # Î¿ Î¯Î´Î¹Î¿Ï‚ Î¿ Ï€Î¯Î½Î±ÎºÎ±Ï‚ -> Ï€ÏÎ¿ÏƒÏ„Î¯Î¸ÎµÏ„Î±Î¹ **Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿Ï‚** Ï‰Ï‚ Î­Î½Î± chunk
        table_block = m.group(0).strip()
        if table_block:
            chunks.append(table_block)

        cursor = end

    # Ï„Ï…Ï‡ÏŒÎ½ Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î¿ Î¼ÎµÏ„Î¬ Ï„Î¿Î½ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ Ï€Î¯Î½Î±ÎºÎ±
    tail = section_text[cursor:].strip()
    if tail:
        sentences = re.split(r'(?<=[\.\!\?])\s+', tail)
        cur, cur_count = [], 0
        for s in sentences:
            wcount = len(s.split())
            if cur_count + wcount > max_words and cur:
                chunks.append(" ".join(cur).strip())
                tail2 = " ".join(" ".join(cur).split()[-overlap_words:])
                cur = [tail2, s]
                cur_count = len(tail2.split()) + wcount
            else:
                cur.append(s)
                cur_count += wcount
        if cur:
            chunks.append(" ".join(cur).strip())

    # Î±Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÎ¬ Î® ÎºÎµÎ½Î¬
    chunks = [c for c in chunks if len(c.split()) > 5]
    return chunks


def load_docs():
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: chunks_list, metadata_list (ordered lists)
    metadata entries: {"filename": fname, "section_title": title, "section_idx": i_section, "chunk_id": j_chunk}
    """
    metadata = []
    all_chunks = []
    for fname in os.listdir(DOCS_PATH):
        if not fname.lower().endswith(".docx"):
            continue
        path = os.path.join(DOCS_PATH, fname)
        sections = read_docx_sections(path)
        for si, sec in enumerate(sections):
            sec_title = sec.get("title")
            sec_text = sec.get("text") or ""
            # split section to chunks
            chunks = chunk_section_text(sec_text, max_words=CHUNK_SIZE, overlap_words=CHUNK_OVERLAP)
            if not chunks:
                # if section was too small, keep whole section text
                if sec_text.strip():
                    chunks = [sec_text.strip()]
            for cj, chunk in enumerate(chunks):
                metadata.append({
                    "filename": fname,
                    "section_title": sec_title,
                    "section_idx": si,
                    "chunk_id": cj,
                    "text": chunk
                })
                all_chunks.append(chunk)
    return all_chunks, metadata


def split_by_headings(text):
    """
    Î£Ï€Î¬ÎµÎ¹ Ï„Î¿ docx ÏƒÎµ ÎµÎ½ÏŒÏ„Î·Ï„ÎµÏ‚ Î¼Îµ Î²Î¬ÏƒÎ· ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚ Ï„ÏÏ€Î¿Ï… '2.4 ...' Î® 'Î†ÏÎ¸ÏÎ¿ ...'
    """
    # ÎšÎ±Î½Î¿Î½Î¹ÎºÎ® Î­ÎºÏ†ÏÎ±ÏƒÎ· Ï€Î¿Ï… ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚ (Ï€.Ï‡. 2.4, 3.1, Î†ÏÎ¸ÏÎ¿ 5, Î˜Î­Î¼Î±)
    pattern = re.compile(r'(?=\n?\s*(?:\d+\.\d+|Î†ÏÎ¸ÏÎ¿\s+\d+|Î˜Î­Î¼Î±|Î•Î½ÏŒÏ„Î·Ï„Î±)\b)', re.IGNORECASE)
    parts = pattern.split(text)
    return [p.strip() for p in parts if len(p.strip()) > 50]  # Î±Î³Î½ÏŒÎ·ÏƒÎµ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÎ¬

def create_faiss_index(embeddings):
    # normalize Î³Î¹Î± cosine similarity
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)   # inner product (cosine if normalized)
    index.add(embeddings)
    return index

def main():
    print("ğŸ“„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")
    chunks, metadata = load_docs()
    print(f"â¡ï¸  Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(chunks)} chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")

    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

    print("ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...")
    embeddings = model.encode(
    [f"passage: {c}" for c in chunks],
    convert_to_numpy=True,
    show_progress_bar=True
    )

    # convert to float32 if ÏŒÏ‡Î¹ Î®Î´Î·
    embeddings = embeddings.astype('float32')

    print("ğŸ”§ ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· embeddings (L2) + Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)

    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")


if __name__ == "__main__":
    main()
