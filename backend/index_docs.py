# backend/index_docs.py
import os
import json
import argparse
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re
import time

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
PDF_PATH = os.path.join(DATA_DIR, "docspdf")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

CHUNK_SIZE = 500
CHUNK_OVERLAP = 150
WORDS_PER_PAGE = 450  # Î³Î¹Î± estimation page mapping

# ============= DOCX Parsing =============
def read_docx_sections(filepath):
    from docx.oxml.text.paragraph import CT_P
    from docx.oxml.table import CT_Tbl
    from docx.text.paragraph import Paragraph
    from docx.table import Table

    doc = Document(filepath)
    sections = []
    current_title = None
    current_body = []

    def flush_section():
        nonlocal current_title, current_body
        if current_body:
            text = "\n".join(current_body).strip()
            if text:
                sections.append({
                    "title": current_title,
                    "text": text
                })
        current_title, current_body = None, []

    for child in doc.element.body:
        if isinstance(child, CT_P):
            p = Paragraph(child, doc)
            txt = p.text.strip()
            if not txt:
                continue
            style = ""
            try:
                style = p.style.name.lower()
            except Exception:
                pass
            if style.startswith("heading") or re.match(r"^\s*(Î¬ÏÎ¸ÏÎ¿|ÎµÎ½ÏŒÏ„Î·Ï„Î±|Î¸Î­Î¼Î±|\d+(\.\d+)+)", txt.lower()):
                flush_section()
                current_title = txt
            else:
                current_body.append(txt)
        elif isinstance(child, CT_Tbl):
            table = Table(child, doc)
            rows = []
            for r in table.rows:
                cells = [c.text.strip().replace("\n", " ") for c in r.cells]
                rows.append(" | ".join(cells))
            if rows:
                current_body.append("ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:\n" + "\n".join(rows))
    flush_section()
    return sections if sections else [{"title": None, "text": "\n".join([p.text for p in doc.paragraphs if p.text.strip()])}]

# ============= Chunking =============
def chunk_text(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    words = text.split()
    chunks = []
    for i in range(0, len(words), size - overlap):
        chunk = " ".join(words[i:i + size])
        if chunk.strip():
            chunks.append(chunk)
        if i + size >= len(words):
            break
    return chunks

# ============= Incremental Indexing =============
def incremental_indexing(model):
    existing_meta = []
    index = None

    if os.path.exists(META_FILE) and os.path.exists(INDEX_FILE):
        with open(META_FILE, "r", encoding="utf-8") as f:
            existing_meta = json.load(f)
        index = faiss.read_index(INDEX_FILE)
        print(f"ğŸ“š Î¥Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î®Î´Î· {len(existing_meta)} ÎºÎ±Ï„Î±Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ FAISS.")
    else:
        print("ğŸ†• Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î¿Ï… FAISS index...")
        index = None
        existing_meta = []

    known_files = {m["filename"] for m in existing_meta}
    current_files = {f for f in os.listdir(DOCS_PATH) if f.endswith(".docx")}
    new_files = current_files - known_files
    removed_files = known_files - current_files

    if not new_files and not removed_files:
        print("âœ… Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Î»Î»Î±Î³Î­Ï‚ ÏƒÏ„Î± Î±ÏÏ‡ÎµÎ¯Î± DOCX. Î¤Î¿ index ÎµÎ¯Î½Î±Î¹ ÎµÎ½Î·Î¼ÎµÏÏ‰Î¼Î­Î½Î¿.")
        return

    if removed_files:
        print(f"ğŸ—‘ï¸ Î”Î¹Î±Î³ÏÎ±Ï†Î® metadata Î³Î¹Î±: {', '.join(removed_files)}")
        existing_meta = [m for m in existing_meta if m["filename"] not in removed_files]
        index = None  # Rebuild all index if deletions occurred

    all_chunks, new_meta = [], []
    start_time = time.time()

    for i, fname in enumerate(sorted(new_files)):
        path = os.path.join(DOCS_PATH, fname)
        sections = read_docx_sections(path)
        pdf_name = Path(fname).stem + ".pdf"
        pdf_path = Path(PDF_PATH) / pdf_name
        pdf_url = f"/pdfs/{pdf_name}" if pdf_path.exists() else None

        print(f"ğŸ“„ [{i+1}/{len(new_files)}] Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±: {fname}")

        for si, sec in enumerate(sections):
            chunks = chunk_text(sec.get("text", ""))
            for cj, chunk in enumerate(chunks):
                words = len(chunk.split())
                new_meta.append({
                    "filename": fname,
                    "section_title": sec.get("title"),
                    "section_idx": si,
                    "chunk_id": cj,
                    "text": chunk,
                    "page_est": max(1, words // WORDS_PER_PAGE),
                    "pdf_link": pdf_url
                })
                all_chunks.append(chunk)

    if not all_chunks:
        print("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î½Î­Î± chunks Î³Î¹Î± indexing.")
        return

    print(f"ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings Î³Î¹Î± {len(all_chunks)} chunks...")
    embeddings = model.encode([f"passage: {c}" for c in all_chunks], convert_to_numpy=True, show_progress_bar=True)
    embeddings = embeddings.astype('float32')
    faiss.normalize_L2(embeddings)

    if index is None:
        print("ğŸ”§ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î¿Ï… FAISS index...")
        dim = embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(embeddings)
        merged_meta = existing_meta + new_meta
    else:
        print("â• Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î½Î­Ï‰Î½ vectors ÏƒÏ„Î¿ Ï…Ï€Î¬ÏÏ‡Î¿Î½ index...")
        index.add(embeddings)
        merged_meta = existing_meta + new_meta

    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(merged_meta, f, ensure_ascii=False, indent=2)

    elapsed = round(time.time() - start_time, 2)
    print(f"âœ… Incremental indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ({elapsed}s).")
    print(f"ğŸ“ˆ ÎÎ­Î± Î±ÏÏ‡ÎµÎ¯Î±: {len(new_files)} | Î”Î¹Î±Î³ÏÎ±Ï†Î­Ï‚: {len(removed_files)} | Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬: {len(merged_meta)} chunks.")

# ============= Main =============
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--reset", action="store_true", help="Î‘Î½Î±Î³ÎºÎ±ÏƒÏ„Î¹ÎºÏŒ rebuild Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏÏ‡Î®")
    args = parser.parse_args()

    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

    if args.reset:
        print("â™»ï¸ Î•Ï€Î±Î½Î±Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€Î»Î®ÏÎ¿Ï…Ï‚ index Î±Ï€ÏŒ Ï„Î¿ Î¼Î·Î´Î­Î½...")
        if os.path.exists(INDEX_FILE): os.remove(INDEX_FILE)
        if os.path.exists(META_FILE): os.remove(META_FILE)

    incremental_indexing(model)

if __name__ == "__main__":
    main()
