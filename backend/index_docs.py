# backend/index_docs.py
import os
import json
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")

CHUNK_SIZE = 350
CHUNK_OVERLAP = 50


# âœ… ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ Markdown Î¼Îµ wrap Î¼ÎµÎ³Î¬Î»Ï‰Î½ ÎºÎµÎ»Î¹ÏÎ½ Î³Î¹Î± ReactMarkdown
def table_to_markdown(table, wrap_length=80):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î­Î½Î±Î½ DOCX Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ Markdown, ÏƒÏ€Î¬Î¶Î¿Î½Ï„Î±Ï‚ Î¼ÎµÎ³Î¬Î»Î± ÎºÎµÎ»Î¹Î¬ Î³Î¹Î± ReactMarkdown.
    wrap_length: Î¼Î­Î³Î¹ÏƒÏ„Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÏ‰Î½ Î±Î½Î¬ Î³ÏÎ±Î¼Î¼Î® ÎºÎµÎ»Î¹Î¿Ï
    """
    def wrap_text(text, max_length=wrap_length):
        words = text.split()
        lines = []
        current = ""
        for word in words:
            if len(current) + len(word) + 1 > max_length:
                lines.append(current)
                current = word
            else:
                current += (" " if current else "") + word
        if current:
            lines.append(current)
        return "<br>".join(lines)  # ReactMarkdown ÎºÎ±Ï„Î±Î»Î±Î²Î±Î¯Î½ÎµÎ¹ <br> Î³Î¹Î± newline

    rows_text = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            text = cell.text.strip()
            text = text.replace("\u00A0", " ").replace("\r", "").replace("\n", " ")
            text = wrap_text(text)
            cells.append(text)
        rows_text.append(" | ".join(cells))

    if not rows_text:
        return ""

    num_cols = rows_text[0].count("|") + 1
    separator = " | ".join(["---"] * num_cols)

    markdown_table = "\n".join([
        "",
        "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:",
        rows_text[0],
        separator,
        *rows_text[1:],
        ""
    ])

    return markdown_table

def read_docx_sections(filepath):
    """Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ DOCX ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î»Î¯ÏƒÏ„Î± ÎµÎ½Î¿Ï„Î®Ï„Ï‰Î½ Î¼Îµ Ï€Î¯Î½Î±ÎºÎµÏ‚ ÎºÎ±Î¹ Ï„Î¯Ï„Î»Î¿Ï…Ï‚."""
    doc = Document(filepath)
    sections = []
    current_title = None
    current_body = []

    def flush_section():
        if not current_title and not current_body:
            return
        text = "\n".join([t.strip() for t in current_body if t.strip()])
        sections.append({
            "title": current_title.strip() if current_title else None,
            "text": text.strip()
        })

    for element in doc.element.body:
        if element.tag.endswith("p"):
            paragraph = doc.paragraphs[len([e for e in doc.element.body if e.tag.endswith('p')]) - len(doc.element.body) + list(doc.element.body).index(element)]
            txt = paragraph.text.strip()
            if not txt:
                continue

            style_name = getattr(paragraph.style, "name", "").lower()
            if style_name.startswith("heading") or "ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´Î±" in style_name:
                flush_section()
                current_title = txt
                current_body = []
                continue

            if re.match(r"^\s*(\d+(\.\d+)+|Î¬ÏÎ¸ÏÎ¿\s+\d+|Î¸Î­Î¼Î±|ÎµÎ½ÏŒÏ„Î·Ï„Î±)", txt.lower()):
                flush_section()
                current_title = txt
                current_body = []
                continue

            current_body.append(txt)

        elif element.tag.endswith("tbl"):
            table = None
            try:
                table = [t for t in doc.tables][len([e for e in doc.element.body if e.tag.endswith("tbl")]) - len(sections) - 1]
            except Exception:
                continue
            if table:
                table_md = table_to_markdown(table)
                if table_md.strip():
                    # ÏŒÏ„Î±Î½ Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹Ï‚ table_text:
                    print("ğŸ“˜ --- TABLE DEBUG ---")
                    print(table_md)
                    print("\n----------------------\n")
                    current_body.append(table_md)

    flush_section()

    if not sections:
        all_text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        sections = [{"title": None, "text": all_text}]

    return sections


def chunk_section_text(section_text, max_words=400, overlap_words=60):
    """Î£Ï€Î¬ÎµÎ¹ ÎµÎ½ÏŒÏ„Î·Ï„ÎµÏ‚ ÏƒÎµ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹Î±, ÎºÏÎ±Ï„ÏÎ½Ï„Î±Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Î±ÎºÎ­ÏÎ±Î¹ÎµÏ‚."""
    if not section_text:
        return []

    sentences = re.split(r'(?<=[\.\!\?])\s+', section_text.strip())
    chunks, cur, cur_count = [], [], 0

    for s in sentences:
        words = s.split()
        wcount = len(words)
        if cur_count + wcount > max_words and cur:
            chunks.append(" ".join(cur).strip())
            tail = " ".join(" ".join(cur).split()[-overlap_words:])
            cur = [tail, s]
            cur_count = len(tail.split()) + wcount
        else:
            cur.append(s)
            cur_count += wcount

    if cur:
        chunks.append(" ".join(cur).strip())

    return [c for c in chunks if len(c.split()) > 5]


def load_docs():
    """Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± DOCX ÎºÎ±Î¹ Ï†Ï„Î¹Î¬Ï‡Î½ÎµÎ¹ chunks Î¼Îµ metadata."""
    metadata, all_chunks = [], []
    for fname in os.listdir(DOCS_PATH):
        if not fname.lower().endswith(".docx"):
            continue
        path = os.path.join(DOCS_PATH, fname)
        sections = read_docx_sections(path)
        for si, sec in enumerate(sections):
            sec_title = sec.get("title")
            sec_text = sec.get("text") or ""
            chunks = chunk_section_text(sec_text, max_words=CHUNK_SIZE, overlap_words=CHUNK_OVERLAP)
            if not chunks and sec_text.strip():
                chunks = [sec_text.strip()]
            for cj, chunk in enumerate(chunks):
                metadata.append({
                    "filename": fname,
                    "section_title": sec_title,
                    "section_idx": si,
                    "chunk_id": cj,
                    "text": chunk
                })
                all_chunks.append(chunk)
    return all_chunks, metadata


def create_faiss_index(embeddings):
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index


def main():
    print("ğŸ“„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")
    chunks, metadata = load_docs()
    print(f"â¡ï¸  Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(chunks)} chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")

    print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")

    print("\n==== SAMPLE METADATA (Ï€ÏÏÏ„Î± 10) ====")
    for i, m in enumerate(metadata[:10]):
        print(f"[{i}] file={m['filename']} section_idx={m['section_idx']} chunk_id={m['chunk_id']}")
        print("TEXT PREVIEW:", m['text'][:200].replace("\n", " "))
        print("---")
    print("Î£ÏÎ½Î¿Î»Î¿ chunks:", len(metadata))

    print("ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...")
    embeddings = model.encode(
        [f"passage: {c}" for c in chunks],
        convert_to_numpy=True,
        show_progress_bar=True
    ).astype('float32')

    print("ğŸ”§ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)

    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")


if __name__ == "__main__":
    main()
