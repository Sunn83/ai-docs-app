import os
import json
import hashlib
import subprocess
import re
from pathlib import Path
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import fitz  # PyMuPDF

DATA_DIR = "/data"
DOCS_PATH = os.path.join(DATA_DIR, "docs")
PDF_PATH = os.path.join(DATA_DIR, "pdfs")
INDEX_FILE = os.path.join(DATA_DIR, "faiss.index")
META_FILE = os.path.join(DATA_DIR, "docs_meta.json")
CACHE_FILE = os.path.join(DATA_DIR, "cache_info.json")

CHUNK_SIZE = 500
CHUNK_OVERLAP = 150


# ---------------------------------------------------
# ğŸ”¹ Helper Î³Î¹Î± Ï€Î¯Î½Î±ÎºÎµÏ‚ ÏƒÎµ Markdown
# ---------------------------------------------------
def table_to_markdown(table, wrap_length=90):
    def wrap_text(text, max_length=wrap_length):
        words = text.split()
        lines, current = [], ""
        for word in words:
            if len(current) + len(word) + 1 > max_length:
                lines.append(current)
                current = word
            else:
                current += (" " if current else "") + word
        if current:
            lines.append(current)
        return " ".join(lines)

    rows_text = []
    for row in table.rows:
        cells = []
        for cell in row.cells:
            text = cell.text.strip().replace("\u00A0", " ").replace("\r", "").replace("\n", " ")
            text = wrap_text(text)
            cells.append(text)
        rows_text.append(" | ".join(cells))

    if not rows_text:
        return ""

    num_cols = rows_text[0].count("|") + 1
    separator = " | ".join(["---"] * num_cols)
    return "\n".join(["", "ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:", rows_text[0], separator, *rows_text[1:], ""])


# ---------------------------------------------------
# ğŸ”¹ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ hash Î³Î¹Î± caching
# ---------------------------------------------------
def file_hash(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


# ---------------------------------------------------
# ğŸ”¹ Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ sections Î±Ï€ÏŒ DOCX
# ---------------------------------------------------
def read_docx_sections(filepath):
    from docx.oxml.text.paragraph import CT_P
    from docx.oxml.table import CT_Tbl
    from docx.text.paragraph import Paragraph
    from docx.table import Table

    doc = Document(filepath)
    sections, current_body = [], []
    current_title = None

    def get_paragraph_text_with_breaks(paragraph):
        parts = []
        for run in paragraph.runs:
            if run.text:
                parts.append(run.text)
            for _ in run._element.findall(".//w:br", namespaces=run._element.nsmap):
                parts.append("\n")
        return "".join(parts).replace("\u00A0", " ").replace("\r", "").strip()

    def flush_section():
        nonlocal current_title, current_body
        if not current_body:
            return
        text = "\n\n".join([t.strip() for t in current_body if t.strip()])
        if text:
            sections.append({"title": current_title.strip() if current_title else None, "text": text})
        current_title, current_body = None, []

    for child in doc.element.body:
        if isinstance(child, CT_P):
            paragraph = Paragraph(child, doc)
            txt = get_paragraph_text_with_breaks(paragraph)
            if not txt:
                continue
            style = paragraph.style.name.lower() if paragraph.style else ""
            if style.startswith("heading") or re.match(r"^\s*(Î¬ÏÎ¸ÏÎ¿|ÎµÎ½ÏŒÏ„Î·Ï„Î±|Î¸Î­Î¼Î±|\d+(\.\d+)+)", txt.lower()):
                flush_section()
                current_title = txt
                continue
            current_body.append(txt)
        elif isinstance(child, CT_Tbl):
            table = Table(child, doc)
            current_body.append(table_to_markdown(table))

    flush_section()
    return sections or [{"title": None, "text": "\n".join(p.text for p in doc.paragraphs if p.text.strip())}]


# ---------------------------------------------------
# ğŸ”¹ Î£Ï€Î¬ÏƒÎ¹Î¼Î¿ ÏƒÎµ chunks
# ---------------------------------------------------
def chunk_section_text(text, max_words=500, overlap_words=100):
    if not text:
        return []
    parts = re.split(r'(?=ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:)', text)
    chunks = []
    prev_part = ""
    join_triggers = ["Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î¯Î½Î±ÎºÎ±Ï‚", "ÎºÎ¬Ï„Ï‰Î¸Î¹ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€Î¯Î½Î±ÎºÎ±", "Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿ Ï€Î¯Î½Î±ÎºÎ±", "Î²Î»Î­Ï€Îµ Ï€Î¯Î½Î±ÎºÎ±", "Ï€Î¯Î½Î±ÎºÎ±:"]

    for part in parts:
        part = part.strip()
        if not part:
            continue

        if part.startswith("ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚:"):
            if prev_part and any(trig in prev_part.lower() for trig in join_triggers):
                chunks[-1] = chunks[-1].rstrip() + "\n\n" + part
                prev_part = ""
            else:
                chunks.append(part)
            continue

        sentences = re.split(r'(?<=[.!?])\s+', part)
        cur, cur_count = [], 0
        for s in sentences:
            wcount = len(s.split())
            if cur_count + wcount > max_words and cur:
                joined = " ".join(cur).strip()
                chunks.append(joined)
                tail = " ".join(" ".join(cur).split()[-overlap_words:])
                cur = [tail, s]
                cur_count = len(tail.split()) + wcount
            else:
                cur.append(s)
                cur_count += wcount
        if cur:
            joined = " ".join(cur).strip()
            chunks.append(joined)
            prev_part = joined

    return [c for c in chunks if len(c.split()) > 5]


# ---------------------------------------------------
# ğŸ”¹ ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® DOCX â†’ PDF
# ---------------------------------------------------
def convert_to_pdf(docx_path, pdf_dir):
    os.makedirs(pdf_dir, exist_ok=True)
    pdf_file = os.path.join(pdf_dir, Path(docx_path).stem + ".pdf")
    if not os.path.exists(pdf_file):
        print(f"âš™ï¸ ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ PDF: {os.path.basename(docx_path)} ...")
        subprocess.run(["libreoffice", "--headless", "--convert-to", "pdf", "--outdir", pdf_dir, docx_path], check=True)
    return pdf_file


# ---------------------------------------------------
# ğŸ”¹ Î’ÏÎ¯ÏƒÎºÎµÎ¹ ÏƒÎµÎ»Î¯Î´Î± ÏƒÏ„Î¿ PDF
# ---------------------------------------------------
def get_page_for_text(pdf_path, text_snippet, cache):
    if pdf_path in cache:
        for snippet, page in cache[pdf_path].items():
            if snippet == text_snippet[:150]:
                return page
    try:
        doc = fitz.open(pdf_path)
        snippet = text_snippet[:150]
        for page_num, page in enumerate(doc, start=1):
            if snippet in page.get_text("text"):
                cache.setdefault(pdf_path, {})[snippet] = page_num
                return page_num
        return 1
    except Exception:
        return 1


# ---------------------------------------------------
# ğŸ”¹ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· DOCX Î¼Îµ caching
# ---------------------------------------------------
def load_docs(cache):
    metadata, all_chunks = [], []
    os.makedirs(DATA_DIR, exist_ok=True)
    doc_files = [f for f in os.listdir(DOCS_PATH) if f.lower().endswith(".docx")]
    print(f"ğŸ” Î•Î½Ï„Î¿Ï€Î¯ÏƒÏ„Î·ÎºÎ±Î½ {len(doc_files)} Î±ÏÏ‡ÎµÎ¯Î± DOCX")

    cached_hashes = cache.get("file_hashes", {})
    new_hashes = {}

    for fname in doc_files:
        path = os.path.join(DOCS_PATH, fname)
        filehash = file_hash(path)
        new_hashes[fname] = filehash
        pdf_path = convert_to_pdf(path, PDF_PATH)

        # Skip Î±Î½ Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ Î±Î»Î»Î¬Î¾ÎµÎ¹
        if cached_hashes.get(fname) == filehash:
            print(f"â© Î Î±ÏÎ¬ÎºÎ±Î¼ÏˆÎ· (Ï‡Ï‰ÏÎ¯Ï‚ Î±Î»Î»Î±Î³Î­Ï‚): {fname}")
            continue

        print(f"ğŸ“˜ Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±: {fname}")
        sections = read_docx_sections(path)
        for si, sec in enumerate(sections):
            chunks = chunk_section_text(sec["text"], max_words=CHUNK_SIZE, overlap_words=CHUNK_OVERLAP)
            for cj, chunk in enumerate(chunks):
                page = get_page_for_text(pdf_path, chunk, cache.get("page_cache", {}))
                metadata.append({
                    "filename": fname,
                    "pdf_path": pdf_path,
                    "section_title": sec.get("title"),
                    "section_idx": si,
                    "chunk_id": cj,
                    "page": page,
                    "text": chunk
                })
                all_chunks.append(chunk)

    cache["file_hashes"] = new_hashes
    return all_chunks, metadata, cache


# ---------------------------------------------------
# ğŸ”¹ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index
# ---------------------------------------------------
def create_faiss_index(embeddings):
    faiss.normalize_L2(embeddings)
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    return index


# ---------------------------------------------------
# ğŸ”¹ Main
# ---------------------------------------------------
def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--rebuild", action="store_true", help="Î Î»Î®ÏÎµÏ‚ indexing ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½")
    args = parser.parse_args()

    cache = {}
    if os.path.exists(CACHE_FILE) and not args.rebuild:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            cache = json.load(f)

    print("ğŸ” Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· indexing...")
    chunks, metadata, cache = load_docs(cache)

    if not chunks:
        print("âœ… Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î½Î­Î± Î® Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î±. Î¤Î¯Ï€Î¿Ï„Î± Ï€ÏÎ¿Ï‚ ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·.")
        return

    print(f"â¡ï¸ {len(chunks)} Î½Î­Î± chunks Ï€ÏÎ¿Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")
    model = SentenceTransformer("intfloat/multilingual-e5-base", cache_folder="/root/.cache/huggingface")
    embeddings = model.encode([f"passage: {c}" for c in chunks], convert_to_numpy=True, show_progress_bar=True)
    embeddings = embeddings.astype("float32")

    index = create_faiss_index(embeddings)
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

    print("âœ… Indexing Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")


if __name__ == "__main__":
    main()
