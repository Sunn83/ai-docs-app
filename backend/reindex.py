from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from docx import Document
import os
import json

DOCS_DIR = "/data/docs"
INDEX_FILE = "/data/faiss.index"
META_FILE = "/data/docs_meta.json"

print("ğŸ” Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… embeddings...")
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

texts = []
metadata = []

print("ğŸ“„ Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Word...")
for file in os.listdir(DOCS_DIR):
    if file.endswith(".docx"):
        path = os.path.join(DOCS_DIR, file)
        try:
            doc = Document(path)
            for i, para in enumerate(doc.paragraphs):
                if para.text.strip():
                    texts.append(para.text.strip())
                    metadata.append({"file": file, "paragraph": i})
        except Exception as e:
            print(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ {file}: {e}")

print(f"ğŸ§  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings Î³Î¹Î± {len(texts)} Ï€Î±ÏÎ±Î³ÏÎ¬Ï†Î¿Ï…Ï‚...")
embeddings = model.encode(texts, show_progress_bar=True)
embeddings = np.array(embeddings).astype("float32")

print("ğŸ’¾ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FAISS index...")
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, INDEX_FILE)

print("ğŸ“ Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· metadata...")
with open(META_FILE, "w", encoding="utf-8") as f:
    json.dump({"texts": texts, "metadata": metadata}, f, ensure_ascii=False, indent=2)

print("âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î¿Ï… FAISS index!")
