version: "3.9"

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ai-docs-app-frontend
    ports:
      - "3000:3000"
    env_file:
      - .env
    depends_on:
      - backend

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ai-docs-app-backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    image: ai-docs-app-backend
    volumes:
      - ./backend:/app
      - ./data:/data
      - ./hf_cache:/root/.cache/huggingface
    restart: unless-stopped
    depends_on:
      llama:
        condition: service_healthy


  llama:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: local-llama
    volumes:
      - ./models:/models
    command:
      - "/server"
      - "--model"
      - "/models/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"
      - "--ctx-size"
      - "4096"
      - "--threads"
      - "8"
      - "--port"
      - "8080"
    ports:
      - "8080:8080"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 15s
      timeout: 5s
      retries: 5

  nginx:
    image: nginx:latest
    container_name: ai-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
    depends_on:
      - frontend
      - backend
